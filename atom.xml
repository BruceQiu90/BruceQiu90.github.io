<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>布鲁斯IO</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-13T07:28:06.872Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Bruce Qiu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark Streaming 通用模版类，避免重复代码</title>
    <link href="http://yoursite.com/2019/07/13/Spark-Streaming-%E9%80%9A%E7%94%A8%E6%A8%A1%E7%89%88%E7%B1%BB%EF%BC%8C%E9%81%BF%E5%85%8D%E9%87%8D%E5%A4%8D%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2019/07/13/Spark-Streaming-通用模版类，避免重复代码/</id>
    <published>2019-07-13T07:06:29.000Z</published>
    <updated>2019-07-13T07:28:06.872Z</updated>
    
    <content type="html"><![CDATA[<p>我的日常工作就是开发 Spark Streaming 实时程序，计算指标，最后通过报表系统展现指标。由于目前公司处于发展阶段，实时程序的需求量不想大公司那样多，而且平台部人力有限，所以目前还是处于来一个实时任务开发一个指标的这么一个阶段。但是写多了实时程序后发现，Spark Streaming 的启动，初始化，解析 kafka 的中的 protobuf 数据。这些都是通用的代码，没有必要反复重复的复制粘贴。这篇文章就记录下利用抽象模版模式，简化 Spark Streaming 的开发过程，让程序员专注业务开发。</p><a id="more"></a><p>废话不多说，首先说下设计思路。利用设计模式中的抽象模版模式，定义一个抽象类，将 spark streaming 的初始化过程全部放到抽象类中，然后定一个 runjob 的抽象方法，这是一个钩子方法，由子类去实现。</p><p>核心代码如下，代码基于 Spark 2.4 进行开发，kafka 版本为 2.1.1</p><p>抽象模版类：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractStreamLauncher</span>(<span class="params">duration: <span class="type">Int</span>, topic: <span class="type">String</span>, groupId: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Launcher</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>(rdd: <span class="type">RDD</span>[<span class="type">Log</span>]): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化</span></span><br><span class="line">  <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span></span><br><span class="line">    .builder()</span><br><span class="line">    .config(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">    .appName(<span class="string">s"<span class="subst">$&#123;this.getClass.getSimpleName&#125;</span>"</span>)</span><br><span class="line">    .enableHiveSupport()</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(spark.sparkContext, <span class="type">Minutes</span>(duration))</span><br><span class="line">  ssc.sparkContext.setLogLevel(<span class="string">"WARN"</span>)</span><br><span class="line">  <span class="keyword">val</span> config: <span class="type">Config</span> = <span class="type">ConfigFactory</span>.load()</span><br><span class="line">  <span class="keyword">val</span> topics: <span class="type">Set</span>[<span class="type">String</span>] = <span class="type">Set</span>(topic)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> kafkaParams = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Object</span>]()</span><br><span class="line">  <span class="comment">//Kafka服务监听端口</span></span><br><span class="line">  kafkaParams.put(<span class="string">"bootstrap.servers"</span>, config.getString(<span class="string">"kafka.broker.list"</span>))</span><br><span class="line">  <span class="comment">//指定kafka输出key的数据类型及编码格式</span></span><br><span class="line">  kafkaParams.put(<span class="string">"key.deserializer"</span>, classOf[<span class="type">StringDeserializer</span>])</span><br><span class="line">  <span class="comment">//指定kafka输出value的数据类型及编码格式</span></span><br><span class="line">  kafkaParams.put(<span class="string">"value.deserializer"</span>, classOf[<span class="type">ProtoBufDeserializer</span>])</span><br><span class="line">  <span class="comment">//消费者ID，随意指定</span></span><br><span class="line">  kafkaParams.put(<span class="string">"group.id"</span>, groupId)</span><br><span class="line">  <span class="comment">//指定从latest(最新,其他版本的是largest这里不行)还是smallest(最早)处开始读取数据</span></span><br><span class="line">  kafkaParams.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>)</span><br><span class="line">  <span class="comment">//如果true,consumer定期地往zookeeper写入每个分区的offset</span></span><br><span class="line">  kafkaParams.put(<span class="string">"enable.auto.commit"</span>, <span class="literal">false</span>:java.lang.<span class="type">Boolean</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* 自动确认offset的时间间隔  */</span></span><br><span class="line">  kafkaParams.put(<span class="string">"auto.commit.interval.ms"</span>, config.getString(<span class="string">"auto.commit.interval.ms"</span>))</span><br><span class="line">  kafkaParams.put(<span class="string">"session.timeout.ms"</span>, config.getString(<span class="string">"session.timeout.ms"</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">//消息发送的最长等待时间.需大于session.timeout.ms这个时间</span></span><br><span class="line">  kafkaParams.put(<span class="string">"request.timeout.ms"</span>, config.getString(<span class="string">"request.timeout.ms"</span>))</span><br><span class="line">  <span class="comment">//一次从kafka中poll出来的数据条数</span></span><br><span class="line">  <span class="comment">//max.poll.records条数据需要在在session.timeout.ms这个时间内处理完</span></span><br><span class="line">  kafkaParams.put(<span class="string">"max.poll.records"</span>, config.getString(<span class="string">"max.poll.records"</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> kafkaStream = <span class="type">KafkaUtils</span>.createDirectStream(ssc,</span><br><span class="line">    <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">    <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">InnoReport</span>.<span class="type">InnoReportLog</span>](topics, kafkaParams))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(): <span class="type">Unit</span> =&#123;</span><br><span class="line">    kafkaStream.foreachRDD(kafkaRDD =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> offsetRanges = kafkaRDD.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">      <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">InnoReport</span>.<span class="type">InnoReportLog</span>] = kafkaRDD.map(_.value())</span><br><span class="line">      <span class="keyword">val</span> value: <span class="type">RDD</span>[<span class="type">Log</span>] = extractLog(rdd)</span><br><span class="line">      runJob(value)</span><br><span class="line">      <span class="comment">// commit offset to kafka</span></span><br><span class="line">      kafkaStream.asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetRanges)</span><br><span class="line">    &#125;)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">extractLog</span></span>(rdd: <span class="type">RDD</span>[<span class="type">InnoReport</span>.<span class="type">InnoReportLog</span>]): <span class="type">RDD</span>[<span class="type">Log</span>] = &#123;</span><br><span class="line">    rdd.map(log =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> fields = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">ValueType</span>]()</span><br><span class="line">      <span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line">      <span class="keyword">val</span> mapList = log.getField.getMapList.asScala</span><br><span class="line">      <span class="keyword">if</span> (mapList != <span class="literal">null</span> &amp;&amp; mapList.nonEmpty) &#123;</span><br><span class="line">        mapList.foreach(m =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> key = m.getKey</span><br><span class="line">          <span class="keyword">if</span> (m.getValue != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">val</span> v = <span class="type">ValueType</span>(m.getValue.getIntType, m.getValue.getLongType, m.getValue.getFloatType, m.getValue.getStringType)</span><br><span class="line">            fields += (key -&gt; v)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> thedate = <span class="type">DateUtils</span>.parseTimestampString(log.getTimestamp)</span><br><span class="line">      <span class="type">Log</span>(log.getIp, log.getTimestamp, fields.toMap, thedate)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面编写子类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RealTask</span>(<span class="params">duration: <span class="type">Int</span>, topic: <span class="type">String</span>, groupId: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">AbstractStreamLauncher</span>(<span class="params">duration, topic, groupId</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>(rdd: <span class="type">RDD</span>[<span class="type">Log</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> df = rdd.toDF()</span><br><span class="line">    <span class="comment">// 进行业务逻辑处理</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RealTask</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">RealTask</span>(args(<span class="number">0</span>).toInt, args(<span class="number">1</span>), args(<span class="number">2</span>)).execute()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到子类很简单，只需要在 runjob 里写相应的业务逻辑即可。</p><p>这样做的好处是：</p><ul><li>代码变的更简洁了，业务代码里只有业务相关的代码，没有繁琐的初始化和读取 kafka 的逻辑。</li><li>便于新入职的同事尽快投入到开发中去，试想一下，一个新同事入职后，熟悉公司的环境也是需要一定时间的。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我的日常工作就是开发 Spark Streaming 实时程序，计算指标，最后通过报表系统展现指标。由于目前公司处于发展阶段，实时程序的需求量不想大公司那样多，而且平台部人力有限，所以目前还是处于来一个实时任务开发一个指标的这么一个阶段。但是写多了实时程序后发现，Spark Streaming 的启动，初始化，解析 kafka 的中的 protobuf 数据。这些都是通用的代码，没有必要反复重复的复制粘贴。这篇文章就记录下利用抽象模版模式，简化 Spark Streaming 的开发过程，让程序员专注业务开发。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="设计模式" scheme="http://yoursite.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Spark任务运行缓慢的问题排查</title>
    <link href="http://yoursite.com/2019/07/13/Spark%E4%BB%BB%E5%8A%A1%E8%BF%90%E8%A1%8C%E7%BC%93%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    <id>http://yoursite.com/2019/07/13/Spark任务运行缓慢的问题排查/</id>
    <published>2019-07-13T05:26:35.000Z</published>
    <updated>2019-07-13T07:05:41.113Z</updated>
    
    <content type="html"><![CDATA[<p>在目前的 Spark 编程中，大多数任务几乎都可以使用 dataframe，利用 sql 进行编程。大大加快了开发的效率。但是殊不知，如果 spark sql 没有用好的话，不但不会加快开发的效率，而且会使你的程序变慢。本文就记录一次我在开发 Spark 程序时遇到的问题。</p><a id="more"></a><p>我在 Spark 离线任务的一次开发过程中，需要从 Hive 中读取一个表的数据，进行 rdd 和 sql 的处理，然后写入到另一个 Hive 表中。程序写完后放到 azkaban 上进行调度，发现 web ui 上显示任务已经跑完了，但是程序始终没有退出，执行了一个小时后 spark 程序才真正退出。当时觉得很纳闷，代码已经在测试数据集上跑通了，验证结果没有问题，各种异常和脏数据已经提前过滤掉了，所以感觉上不是业务逻辑的问题。这时候观察最后一个 stage 的执行情况，发现在最后一个 stage，一共生成了有 3 万多个 task，然后在看下最后生成的 Hive 表对应的 HDFS 上的文件，发现目录下不断有文件写入，而且每个文件都非常小，连 1kb 都不到。这下找到原因了，原来是 job 花了大量的时间在写文件上。</p><p>所以，我们在最后往 HDFS 上写文件，或者写 hive 表的时候，一定要把 partiton 的数量降下来，这样一来，既可以避免小文件过多的问题，又可以加快 job 执行的时间。</p><p>核心代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.repartition(<span class="number">1</span>).createOrReplaceTempView(<span class="string">"complete_rate"</span>)</span><br><span class="line"><span class="comment">// partition 数量需要根据最后输出的大小进行调整</span></span><br><span class="line"></span><br><span class="line">      spark.sql(</span><br><span class="line">        <span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">           | insert into rpt_innoreport_realtime.index3</span></span><br><span class="line"><span class="string">           | select deviceId, eventNo, taskId, channel, thedate from complete_rate</span></span><br><span class="line"><span class="string">         "</span><span class="string">""</span>.stripMargin)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在目前的 Spark 编程中，大多数任务几乎都可以使用 dataframe，利用 sql 进行编程。大大加快了开发的效率。但是殊不知，如果 spark sql 没有用好的话，不但不会加快开发的效率，而且会使你的程序变慢。本文就记录一次我在开发 Spark 程序时遇到的问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark多线程并发执行任务</title>
    <link href="http://yoursite.com/2019/07/01/Spark%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%B9%B6%E5%8F%91%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1/"/>
    <id>http://yoursite.com/2019/07/01/Spark多线程并发执行任务/</id>
    <published>2019-07-01T15:26:06.000Z</published>
    <updated>2019-07-13T05:22:58.212Z</updated>
    
    <content type="html"><![CDATA[<p>在我们平时使用 Spark 的过程中，大多数的情况下一次只会提交一个 Job 运行。然后 Job 的内部会根据具体的任务形成对应的 task 来执行任务。例如我最近开发的一个 HDFS 小文件合并的程序，依次读取 HDFS 文件夹下的所有文件，按照 256M 一个文件进行合并。</p><p>思考下，这时 Spark 会加载一个文件夹下的所有的文件，然后根据block个数生成task数目，多个task运行中不同的进程中，是并行的，如果在同一个进程中一个JVM里面有多个task，那么多个task也可以并行，这是常见的使用方式。 </p><a id="more"></a><p>一般情况下这么做是没有问题的，但是我们公司的场景下，由于业务急速扩张，有些业务方在使用 spark 程序时使用不当，导致了 HDFS 小文件迅速扩张。因此必须要加快小文件的合并速度。所以按照传统的编程方式带来的问题就是合并的速度过慢。那么有什么方法可以加速 Spark 任务的执行呢？答案是有的：那就是一次性提交多个任务。</p><p>具体做法如下：在 SparkContext 中使用多线程，一次性批量提交多个任务并行执行。</p><p>核心代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">     .appName(<span class="string">s"<span class="subst">$&#123;this.getClass.getSimpleName&#125;</span>"</span>)</span><br><span class="line">     .config(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">     .enableHiveSupport()</span><br><span class="line">     .getOrCreate()</span><br><span class="line"></span><br><span class="line">   <span class="comment">//保存任务返回值</span></span><br><span class="line">   <span class="keyword">val</span> list=<span class="keyword">new</span> java.util.<span class="type">ArrayList</span>[<span class="type">Future</span>[<span class="type">String</span>]]()</span><br><span class="line">   <span class="comment">//并行任务读取的path</span></span><br><span class="line">   <span class="keyword">val</span> task_paths=<span class="keyword">new</span> java.util.<span class="type">ArrayList</span>[<span class="type">String</span>]()</span><br><span class="line">   task_paths.add(<span class="string">"/tmp/data/path1/"</span>)</span><br><span class="line">   task_paths.add(<span class="string">"/tmp/data/path2/"</span>)</span><br><span class="line">   task_paths.add(<span class="string">"/tmp/data/path3/"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">//线程数等于path的数量</span></span><br><span class="line">   <span class="keyword">val</span> nums_threads=task_paths.size()</span><br><span class="line">   <span class="comment">//构建线程池</span></span><br><span class="line">   <span class="keyword">val</span> executors=<span class="type">Executors</span>.newFixedThreadPool(nums_threads)</span><br><span class="line">   <span class="keyword">for</span>(i&lt;<span class="number">-0</span> until  nums_threads)&#123;</span><br><span class="line">     <span class="keyword">val</span> task= executors.submit(<span class="keyword">new</span> <span class="type">Callable</span>[<span class="type">String</span>] &#123;</span><br><span class="line">       <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">call</span></span>(): <span class="type">String</span> =&#123;</span><br><span class="line">         <span class="keyword">val</span> count=spark.sparkContext.textFile(task_paths.get(i)).count()<span class="comment">//获取统计文件数量</span></span><br><span class="line">         <span class="keyword">return</span> task_paths.get(i)+<span class="string">" 文件数量： "</span>+count</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;)</span><br><span class="line"></span><br><span class="line">     list.add(task)<span class="comment">//添加集合里面</span></span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">//遍历获取结果</span></span><br><span class="line">   <span class="keyword">import</span> scala.collection.<span class="type">JavaConverters</span>._</span><br><span class="line">   list.asScala.foreach(result=&gt;&#123;</span><br><span class="line">     println(result.get())</span><br><span class="line">   &#125;)</span><br><span class="line">   <span class="comment">//停止spark</span></span><br><span class="line">   spark.stop</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>这样就完成了一个并行提交 task 的任务。我们将任务提交到集群，在 Spark UI 中可以看到 Active Job 中一共有三个任务在同时运行。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在我们平时使用 Spark 的过程中，大多数的情况下一次只会提交一个 Job 运行。然后 Job 的内部会根据具体的任务形成对应的 task 来执行任务。例如我最近开发的一个 HDFS 小文件合并的程序，依次读取 HDFS 文件夹下的所有文件，按照 256M 一个文件进行合并。&lt;/p&gt;
&lt;p&gt;思考下，这时 Spark 会加载一个文件夹下的所有的文件，然后根据block个数生成task数目，多个task运行中不同的进程中，是并行的，如果在同一个进程中一个JVM里面有多个task，那么多个task也可以并行，这是常见的使用方式。 &lt;/p&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://yoursite.com/categories/BigData/"/>
    
      <category term="Spark" scheme="http://yoursite.com/categories/BigData/Spark/"/>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="BigData" scheme="http://yoursite.com/tags/BigData/"/>
    
  </entry>
  
</feed>
